{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-12T09:42:49.863476Z","iopub.execute_input":"2023-11-12T09:42:49.864396Z","iopub.status.idle":"2023-11-12T09:42:49.881974Z","shell.execute_reply.started":"2023-11-12T09:42:49.864360Z","shell.execute_reply":"2023-11-12T09:42:49.881100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:42:53.735227Z","iopub.execute_input":"2023-11-12T09:42:53.735569Z","iopub.status.idle":"2023-11-12T09:43:07.093727Z","shell.execute_reply.started":"2023-11-12T09:42:53.735539Z","shell.execute_reply":"2023-11-12T09:43:07.092773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown,zipfile\n\n# Google Drive file ID from the provided URL\nfile_id = \"131VnscdKVOcv6AyDCv4zrTWApHyeuZ4d\"\n# Destination directory for extracted data\noutput_directory = \"/kaggle/working/DATA\"\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n# Define the zip file name\nzip_filename = os.path.join(output_directory, \"train.zip\")\n\n# Download the data from Google Drive\ngdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_filename, quiet=False)\n\n# Check if the downloaded file is a ZIP archive\nif zipfile.is_zipfile(zip_filename):\n    # Extract the data from the zip file\n    with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n        zip_ref.extractall(output_directory)\n    print(\"Data downloaded and extracted successfully.\")\nelse:\n    print(\"The downloaded file is not a valid ZIP archive.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:43:07.095565Z","iopub.execute_input":"2023-11-12T09:43:07.095876Z","iopub.status.idle":"2023-11-12T09:43:09.238054Z","shell.execute_reply.started":"2023-11-12T09:43:07.095850Z","shell.execute_reply":"2023-11-12T09:43:09.237278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown,zipfile\n\n# Google Drive file ID from the provided URL\nfile_id = \"1isg6zgjE3SlwgRYfYn4AvLud6MQj3C_b\"\n# Destination directory for extracted data\noutput_directory = \"/kaggle/working/DATA/images\"\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(output_directory):\n    os.makedirs(output_directory)\n\n# Define the zip file name\nzip_filename = os.path.join(output_directory, \"images.zip\")\n\n# Download the data from Google Drive\ngdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_filename, quiet=False)\n\n# Check if the downloaded file is a ZIP archive\nif zipfile.is_zipfile(zip_filename):\n    # Extract the data from the zip file\n    with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n        zip_ref.extractall(output_directory)\n    print(\"Data downloaded and extracted successfully.\")\nelse:\n    print(\"The downloaded file is not a valid ZIP archive.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:43:09.239640Z","iopub.execute_input":"2023-11-12T09:43:09.240170Z","iopub.status.idle":"2023-11-12T09:43:25.346472Z","shell.execute_reply.started":"2023-11-12T09:43:09.240135Z","shell.execute_reply":"2023-11-12T09:43:25.345403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n# Install dew required libraries\n!pip install pyunpack\n!pip install patool\n!pip install rasterio\nclear_output()\n# Import libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport geopandas as gpd\nimport rasterio\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:43:25.348981Z","iopub.execute_input":"2023-11-12T09:43:25.349695Z","iopub.status.idle":"2023-11-12T09:44:06.353271Z","shell.execute_reply.started":"2023-11-12T09:43:25.349641Z","shell.execute_reply":"2023-11-12T09:44:06.352120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load files\npath_tabular_data= \"/kaggle/input/tabdata/\"\n\ndf_train = gpd.read_file(f\"{path_tabular_data}train.shp\")\ndf_test = gpd.read_file(f\"{path_tabular_data}test.shp\")\nsamplesubmission = pd.read_csv(f\"{path_tabular_data}SampleSubmission.csv\")\n\n# Preview train dataset\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:44:06.354809Z","iopub.execute_input":"2023-11-12T09:44:06.355196Z","iopub.status.idle":"2023-11-12T09:44:07.396570Z","shell.execute_reply.started":"2023-11-12T09:44:06.355146Z","shell.execute_reply":"2023-11-12T09:44:07.395375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:41:56.506025Z","iopub.execute_input":"2023-11-11T22:41:56.506360Z","iopub.status.idle":"2023-11-11T22:41:56.575854Z","shell.execute_reply.started":"2023-11-11T22:41:56.506332Z","shell.execute_reply":"2023-11-11T22:41:56.574891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:41:56.577081Z","iopub.execute_input":"2023-11-11T22:41:56.577391Z","iopub.status.idle":"2023-11-11T22:41:56.590884Z","shell.execute_reply.started":"2023-11-11T22:41:56.577364Z","shell.execute_reply":"2023-11-11T22:41:56.589949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samplesubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:41:56.592079Z","iopub.execute_input":"2023-11-11T22:41:56.592434Z","iopub.status.idle":"2023-11-11T22:41:56.603304Z","shell.execute_reply.started":"2023-11-11T22:41:56.592397Z","shell.execute_reply":"2023-11-11T22:41:56.602243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape, df_test.shape, samplesubmission.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:41:56.605115Z","iopub.execute_input":"2023-11-11T22:41:56.605961Z","iopub.status.idle":"2023-11-11T22:41:56.615787Z","shell.execute_reply.started":"2023-11-11T22:41:56.605918Z","shell.execute_reply":"2023-11-11T22:41:56.614881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(df_train, x=\"target\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:41:56.620219Z","iopub.execute_input":"2023-11-11T22:41:56.621110Z","iopub.status.idle":"2023-11-11T22:41:57.059670Z","shell.execute_reply.started":"2023-11-11T22:41:56.621081Z","shell.execute_reply":"2023-11-11T22:41:57.058626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_satellite__data= \"/kaggle/working/DATA/images/GeoAIHack2023/\"\nimages_list= os.listdir(path_satellite__data)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:44:07.398157Z","iopub.execute_input":"2023-11-12T09:44:07.398627Z","iopub.status.idle":"2023-11-12T09:44:07.404456Z","shell.execute_reply.started":"2023-11-12T09:44:07.398587Z","shell.execute_reply":"2023-11-12T09:44:07.403274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open 1 timestamp example image\nimage_path = f\"{path_satellite__data}{images_list[23]}\"\nwith rasterio.open(image_path) as src:\n    # Get the image metadata\n    metadata = src.meta\n    # Read the image as an array\n    image = src.read()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:41:57.068777Z","iopub.execute_input":"2023-11-11T22:41:57.069198Z","iopub.status.idle":"2023-11-11T22:41:57.415105Z","shell.execute_reply.started":"2023-11-11T22:41:57.069161Z","shell.execute_reply":"2023-11-11T22:41:57.414072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the Spectral-Bands of the image\n# The Sentinel-2 Spectral band includes \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B11\", \"B12\"\nfig, ax = plt.subplots(2, 5, figsize=(25, 10))\n\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(image[i*4+j,:,:])\n        ax[i,j].set_title(\"Band {}\".format(i*4+j+1))\n        ax[i,j].axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:41:57.416431Z","iopub.execute_input":"2023-11-11T22:41:57.416779Z","iopub.status.idle":"2023-11-11T22:42:01.668812Z","shell.execute_reply.started":"2023-11-11T22:41:57.416751Z","shell.execute_reply":"2023-11-11T22:42:01.667487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Field Visualization to see if there's same distrubtion of the bands over Timestamps","metadata":{}},{"cell_type":"markdown","source":"# visualization of Olive Fileds with different Timestamps","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have a list of timestamp names\ntimestamp_names = [\"Timestamp {}\".format(i) for i in range(24)]\n\n\nfor t in range(24):  # loop over timestamps\n    fig, ax = plt.subplots(4, 7, figsize=(25, 10))\n\n    for i in range(4):\n        for j in range(7):\n            ax[i, j].imshow(new_data[0, t, i * 6 +j , :, :])\n            ax[i, j].set_title(\"Band {}\".format(i * 6 + j + 1))\n            ax[i, j].axis(\"off\")\n    \n    fig.suptitle(timestamp_names[t], fontsize=16)  # Add timestamp name as the overall title\n    plt.show()  # show each timestamp in a separate figure\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:24:06.273084Z","iopub.execute_input":"2023-11-11T18:24:06.274020Z","iopub.status.idle":"2023-11-11T18:24:57.694225Z","shell.execute_reply.started":"2023-11-11T18:24:06.273985Z","shell.execute_reply":"2023-11-11T18:24:57.693231Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualization of Not_Olive Fileds with different Timestamps","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have a list of timestamp names\ntimestamp_names = [\"Timestamp {}\".format(i) for i in range(24)]\n\n\nfor t in range(24):  # loop over timestamps\n    fig, ax = plt.subplots(4, 7, figsize=(25, 10))\n\n    for i in range(4):\n        for j in range(7):\n            ax[i, j].imshow(new_data[1, t, i * 6 +j , :, :])\n            ax[i, j].set_title(\"Band {}\".format(i * 6 + j + 1))\n            ax[i, j].axis(\"off\")\n    \n    fig.suptitle(timestamp_names[t], fontsize=16)  # Add timestamp name as the overall title\n    plt.show()  # show each timestamp in a separate figure\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T18:25:30.445828Z","iopub.execute_input":"2023-11-11T18:25:30.446252Z","iopub.status.idle":"2023-11-11T18:26:21.717176Z","shell.execute_reply.started":"2023-11-11T18:25:30.446220Z","shell.execute_reply":"2023-11-11T18:26:21.716123Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Machine Learning Approach (withe Vegetation Indices,Geometry Features and Statistical Features with all Timestamps)****","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame()\nfor timestamp in range(24):  # Assuming timestamps range from 0 to 24\n    image_path = f\"{path_satellite__data}{images_list[timestamp]}\"\n    \n    with rasterio.open(image_path) as src:\n        # Get the image metadata\n        metadata = src.meta\n        # Read the image as an array\n        image = src.read()\n    # Concatenate the training and testing dataframes into a single dataframe\n    df_all= pd.concat([df_train, df_test], ignore_index=True)\n\n    # Initialize an empty list to store the processed data\n    processed_data= []\n    # Loop through each field in the concatenated dataframe\n    for index, row in df_all.iterrows():\n        # Get the geometry of the field\n        field_geometry = row['geometry']\n        # Create a mask for the field\n        field_mask = geometry_mask([field_geometry], out_shape=src.shape, transform=src.transform, invert=True)\n        # Extract the image data for the field\n        field_data = image[:, field_mask]\n\n        # Compute the mean and standard deviation of each channel of the image data\n        mean_values = np.mean(field_data, axis=1)\n        std_values = np.std(field_data, axis=1)\n        max_values = np.max(field_data, axis=1)\n        min_values = np.min(field_data, axis=1)\n        median_values=np.median(field_data, axis=1)\n\n        # Concatenate the mean and standard deviation values into a single feature vector\n        features = np.concatenate([mean_values, std_values,max_values,min_values,median_values])\n        # Append the feature vector to the list of processed data\n        processed_data.append(features)\n\n    # Convert the list of processed data into a pandas dataframe\n    processed_data_df= pd.DataFrame(processed_data, columns=[f\"mean_channel_{i}\" for i in range(0, 10)] + [f\"std_channel_{i}\" for i in range(0, 10)]+[f\"max_channel_{i}\" for i in range(0, 10)] + [f\"min_channel_{i}\" for i in range(0, 10)]+[f\"median_channel_{i}\" for i in range(0, 10)])\n    data=pd.concat(processed_data_df,axis=1)\ndata[\"ID\"]= df_all[\"ID\"]    ","metadata":{"execution":{"iopub.status.busy":"2023-11-11T23:03:59.868979Z","iopub.execute_input":"2023-11-11T23:03:59.870011Z","iopub.status.idle":"2023-11-11T23:04:16.993995Z","shell.execute_reply.started":"2023-11-11T23:03:59.869970Z","shell.execute_reply":"2023-11-11T23:04:16.992462Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport rasterio\nfrom rasterio.features import geometry_mask\n\n# Initialize an empty dataframe to store the processed data\ndata = pd.DataFrame()\ndf_all= pd.concat([df_train, df_test], ignore_index=True)\n\n# Assuming timestamps range from 0 to 24\nfor timestamp in range(24):\n    image_path = f\"{path_satellite__data}{images_list[timestamp]}\"\n    \n    with rasterio.open(image_path) as src:\n        # Get the image metadata\n        metadata = src.meta\n        # Read the image as an array\n        image = src.read()\n\n    # Initialize an empty list to store the processed data for the current timestamp\n    processed_data = []\n    \n    # Loop through each field in the concatenated dataframe\n    for index, row in df_all.iterrows():\n        # Get the geometry of the field\n        field_geometry = row['geometry']\n        # Create a mask for the field\n        field_mask = geometry_mask([field_geometry], out_shape=src.shape, transform=src.transform, invert=True)\n        # Extract the image data for the field\n        field_data = image[:, field_mask]\n\n        # Compute the mean and standard deviation of each channel of the image data\n        mean_values = np.mean(field_data, axis=1)\n        std_values = np.std(field_data, axis=1)\n        max_values = np.max(field_data, axis=1)\n        min_values = np.min(field_data, axis=1)\n        median_values = np.median(field_data, axis=1)\n\n        # Concatenate the mean and standard deviation values into a single feature vector\n        features = np.concatenate([mean_values, std_values, max_values, min_values, median_values])\n        # Append the feature vector to the list of processed data for the current timestamp\n        processed_data.append(features)\n\n    # Convert the list of processed data into a pandas dataframe for the current timestamp\n    processed_data_df = pd.DataFrame(processed_data, columns=[f\"mean_channel_{i}_timestamp_{timestamp}\" for i in range(0, 10)] + [f\"std_channel_{i}_timestamp_{timestamp}\" for i in range(0, 10)]+[f\"max_channel_{i}_timestamp_{timestamp}\" for i in range(0, 10)] + [f\"min_channel_{i}_timestamp_{timestamp}\" for i in range(0, 10)]+[f\"median_channel_{i}_timestamp_{timestamp}\" for i in range(0, 10)])\n    if timestamp<1:\n        processed_data_df[\"ID\"] = df_all[\"ID\"]\n    else:\n        pass\n    # Concatenate the current timestamp's processed data to the main dataframe\n    data = pd.concat([data, processed_data_df], axis=1)\n\n# You may want to reset the index of the resulting dataframe\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:44:07.406861Z","iopub.execute_input":"2023-11-12T09:44:07.407206Z","iopub.status.idle":"2023-11-12T09:50:38.388631Z","shell.execute_reply.started":"2023-11-12T09:44:07.407173Z","shell.execute_reply":"2023-11-12T09:50:38.387681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Engineering:**\n# 1. Geometry Features\n# 2. Vegetation Indices\n# 3. Statistics on all Timestamps","metadata":{}},{"cell_type":"code","source":"def calculate_polygon_features(polygon):\n    area = polygon.area\n    perimeter = polygon.length\n    centroid = polygon.centroid\n    bounding_box = polygon.bounds\n    aspect_ratio = bounding_box[2] - bounding_box[0] / (bounding_box[3] - bounding_box[1])\n    compactness = 4 * np.pi * area / perimeter**2\n    convex_hull = polygon.convex_hull\n    orientation = np.arctan2(centroid.y - bounding_box[1], centroid.x - bounding_box[0])\n    perimeter_to_area_ratio = perimeter / area\n    circumradius = np.sqrt((bounding_box[2] - bounding_box[0])**2 + (bounding_box[3] - bounding_box[1])**2) / 2\n    inscribed_circle_radius = area / perimeter * 2 / np.pi\n\n    return {\n        'Area': area,\n        'Perimeter': perimeter,\n        'Aspect Ratio': aspect_ratio,\n        'Compactness': compactness,\n        'Orientation': orientation,\n        'Perimeter-to-Area Ratio': perimeter_to_area_ratio,\n        'Circumradius': circumradius,\n        'Inscribed Circle Radius': inscribed_circle_radius\n    }\n\ndata[\"geometry\"]= df_all[\"geometry\"]    \n# Apply the function to the GeoDataFrame\ndata['Polygon_Features'] = data['geometry'].apply(calculate_polygon_features)\n\n# Create new columns for each feature\ndata['Area'] = data['Polygon_Features'].apply(lambda x: x['Area'])\ndata['Perimeter'] = data['Polygon_Features'].apply(lambda x: x['Perimeter'])\ndata['Aspect_Ratio'] = data['Polygon_Features'].apply(lambda x: x['Aspect Ratio'])\ndata['Compactness'] = data['Polygon_Features'].apply(lambda x: x['Compactness'])\ndata['Orientation'] = data['Polygon_Features'].apply(lambda x: x['Orientation'])\ndata['Perimeter_to_Area_Ratio'] = data['Polygon_Features'].apply(lambda x: x['Perimeter-to-Area Ratio'])\ndata['Circumradius'] = data['Polygon_Features'].apply(lambda x: x['Circumradius'])\ndata['Inscribed_Circle_Radius'] = data['Polygon_Features'].apply(lambda x: x['Inscribed Circle Radius'])\n\n# Drop the original Polygon_Features column if needed\ndata = data.drop(columns=['Polygon_Features'])\ndata = data.drop(columns=[\"geometry\"])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:51:46.677400Z","iopub.execute_input":"2023-11-12T09:51:46.678270Z","iopub.status.idle":"2023-11-12T09:51:47.003600Z","shell.execute_reply.started":"2023-11-12T09:51:46.678239Z","shell.execute_reply":"2023-11-12T09:51:47.002672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_vegetation_indices(b2, b3, b4, b5, b6, b7, b8, b8A, b11, b12):\n    indices = {}\n\n    # Common indices\n    indices['NDVI'] = (b8 - b4) / (b8 + b4)\n    indices['EVI'] = 2.5 * (b8 - b4) / (b8 + 6 * b4 - 7.5 * b2 + 1)\n    indices['SAVI'] = (1 + 0.5) * (b8 - b4) / (b8 + b4 + 0.5)\n    indices['MNDWI'] = (b3 - b11) / (b3 + b11)\n    indices['NDII'] = (b8 - b11) / (b8 + b11)\n    indices['NBR'] = (b8 - b12) / (b8 + b12)\n\n    # Additional indices\n    indices['GNDVI'] = (b8 - b3) / (b8 + b3)\n    indices['SR'] = b8 / b4\n    indices['VARI'] = (b3 - b2) / (b3 + b2 - b11)\n    indices['LAI'] = 1/6 * np.log(6 * (b4 - b3) / (b4 + b3))\n    indices['PSRI'] = (b4 - b3) / b6\n    indices['NLI'] = (b8**2 - b4) / (b8**2 + b4)\n\n    # Additional indices continued\n    indices['WSI'] = (b3 - b5) / (b3 + b5)\n    indices['LCI'] = (b8A - b7) / (b8A + b7)\n    indices['CCCI'] = (b8 - b4) / (b8 + b4)\n    indices['RENDVI'] = (b8 - b5) / (b8 + b5)\n    indices['TGI'] = -0.5 * (190 * (b2 - b3) - 120 * (b2 - b4))\n    indices['NPQI'] = (b4 - b2) / (b4 + b2)\n    indices['CRI'] = (b7 - b5) / (b7 + b5)\n    indices['NDRE'] = (b5 - b4) / (b5 + b4)\n    \n    indices['MTVI'] = 1.5 * (1.2 * (b8 - b6) - 2.5 * (b4 - b3))\n    indices['EVI2'] = 2.5 * (b8 - b2) / (b8 + 2.4 * b2 + 1)\n    indices['RENDAI'] = (b7 - b5) / (b7 + b5)\n    indices['MTCI'] = (b7 - b6) / (b6 - b5)\n    indices['VARI'] = (b3 - b2) / (b3 + b2 + b11)\n    indices['LWI'] = (b8A - b11) / (b8A + b11)\n    indices['TSAVI'] = 0.8 * (b8 - b4) / ((b8 + b4 + 0.6) * (1 + 0.6))\n    indices['CIRED2'] = (b8 - b5) / (b7 - b5)\n    indices['VGI'] = b3 / (b2 + b4)\n    indices['NDWI2'] = (b11 - b12) / (b11 + b12)\n\n\n    return indices\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:51:54.613310Z","iopub.execute_input":"2023-11-12T09:51:54.614029Z","iopub.status.idle":"2023-11-12T09:51:54.628059Z","shell.execute_reply.started":"2023-11-12T09:51:54.613998Z","shell.execute_reply":"2023-11-12T09:51:54.626957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_stat = [\"mean\", \"median\", \"max\", \"min\", \"std\"]\nchannels = range(10)  # Assuming channels are from 0 to 9\n\nvegetation_indices = [\"NDVI\",\"EVI\",\"SAVI\",\"MNDWI\",\"NDII\",\"NBR\",\"GNDVI\",\"SR\",\"VARI\",\"LAI\",\"PSRI\",\"NLI\",\"WSI\",\"LCI\",\"CCCI\",\"RENDVI\",\"TGI\",\"NPQI\",\"CRI\",\"NDRE\",\"MTVI\",\"EVI2\",\"RENDAI\",\"MTCI\",\"VARI\",\"LWI\",\"TSAVI\",\"CIRED2\",\"VGI\",\"NDWI2\"]\n\nfor timestamp in range(24):\n    for stat in list_stat:\n        for index_name in vegetation_indices:\n            bands = {\n                'b2': data[f'{stat}_channel_0_timestamp_{timestamp}'],\n                'b3': data[f'{stat}_channel_1_timestamp_{timestamp}'],\n                'b4': data[f'{stat}_channel_2_timestamp_{timestamp}'],\n                'b5': data[f'{stat}_channel_3_timestamp_{timestamp}'],\n                'b6': data[f'{stat}_channel_4_timestamp_{timestamp}'],\n                'b7': data[f'{stat}_channel_5_timestamp_{timestamp}'],\n                'b8': data[f'{stat}_channel_6_timestamp_{timestamp}'],\n                'b8A': data[f'{stat}_channel_7_timestamp_{timestamp}'],\n                'b11': data[f'{stat}_channel_8_timestamp_{timestamp}'],\n                'b12': data[f'{stat}_channel_9_timestamp_{timestamp}'],\n            }\n            index_values = calculate_vegetation_indices(**bands)[index_name]\n\n            # Add the new index columns to the dataframe\n            data[f'{index_name}_{stat}_timestamp_{timestamp}'] = index_values\n\n            \n\n    \n            # Add more conditions for other vegetation indices\n            \n            # Add the new index columns to the dataframe\n            data[f'{index_name}_{stat}_timestamp_{timestamp}'] = index_values\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:52:05.372500Z","iopub.execute_input":"2023-11-12T09:52:05.372843Z","iopub.status.idle":"2023-11-12T09:52:52.865817Z","shell.execute_reply.started":"2023-11-12T09:52:05.372815Z","shell.execute_reply":"2023-11-12T09:52:52.864851Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:54:14.032073Z","iopub.execute_input":"2023-11-12T09:54:14.033036Z","iopub.status.idle":"2023-11-12T09:54:14.039055Z","shell.execute_reply.started":"2023-11-12T09:54:14.033001Z","shell.execute_reply":"2023-11-12T09:54:14.038072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:54:19.542061Z","iopub.execute_input":"2023-11-12T09:54:19.542990Z","iopub.status.idle":"2023-11-12T09:54:19.586386Z","shell.execute_reply.started":"2023-11-12T09:54:19.542959Z","shell.execute_reply":"2023-11-12T09:54:19.585447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.to_csv(\"final_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T04:11:48.144491Z","iopub.execute_input":"2023-11-12T04:11:48.144921Z","iopub.status.idle":"2023-11-12T04:12:12.157808Z","shell.execute_reply.started":"2023-11-12T04:11:48.144888Z","shell.execute_reply":"2023-11-12T04:12:12.156870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrive the train and test sets\nfrom sklearn.model_selection import train_test_split\n\nprocessed_train_df= data.iloc[:len(df_train), :]\nprocessed_test_df= data.iloc[len(df_train):, :]\nprocessed_test_df.reset_index(drop=True, inplace=True)\n\n# Split the dataset into training and testing sets\n\nX = processed_train_df.drop(columns=[\"ID\"])\ny = df_train[\"target\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:54:31.111794Z","iopub.execute_input":"2023-11-12T09:54:31.112643Z","iopub.status.idle":"2023-11-12T09:54:31.863678Z","shell.execute_reply.started":"2023-11-12T09:54:31.112606Z","shell.execute_reply":"2023-11-12T09:54:31.862853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing With RF**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Create an instance of the RandomForestClassifier class\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\nX_train=X_train.fillna(0)\nX_test=X_test.fillna(0)\nX_train.replace([np.inf, -np.inf], 0, inplace=True)\nX_test.replace([np.inf, -np.inf], 0, inplace=True)\n# Fit the model to the training data\nrfc.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = rfc.predict(X_test)\n\n# Evaluate the model's performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-12T05:54:59.073818Z","iopub.execute_input":"2023-11-12T05:54:59.074607Z","iopub.status.idle":"2023-11-12T05:55:03.758948Z","shell.execute_reply.started":"2023-11-12T05:54:59.074568Z","shell.execute_reply":"2023-11-12T05:55:03.757864Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Yeo-Johnson transformation with Catboost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier, Pool, cv, CatBoost\nfrom sklearn.preprocessing import PowerTransformer\nX.replace([np.inf, -np.inf],0, inplace=True)\n#X_test.replace([np.inf, -np.inf], 0, inplace=True)\n\n#power_transformer = PowerTransformer(method='yeo-johnson')\n#X_train_transformed = power_transformer.fit_transform(X_train)\n#X_test_transformed = power_transformer.transform(X_test)\n\n# Create a CatBoost pool for training data\ntrain_pool = Pool(X, y)\n\n# Define CatBoost parameters\n\n\n# Create and train CatBoost classifier\ncatboost_clf = CatBoostClassifier()\ncatboost_clf.fit(train_pool, eval_set=(X_test, y_test), plot=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T06:03:32.754503Z","iopub.execute_input":"2023-11-12T06:03:32.754930Z","iopub.status.idle":"2023-11-12T06:13:40.345748Z","shell.execute_reply.started":"2023-11-12T06:03:32.754894Z","shell.execute_reply":"2023-11-12T06:13:40.344750Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import *\nfrom sklearn.model_selection import *\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:56:29.022827Z","iopub.execute_input":"2023-11-12T09:56:29.023553Z","iopub.status.idle":"2023-11-12T09:56:29.028544Z","shell.execute_reply.started":"2023-11-12T09:56:29.023519Z","shell.execute_reply":"2023-11-12T09:56:29.027583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Lgbm","metadata":{}},{"cell_type":"code","source":"# Fit the model\nmodel = lgb.LGBMClassifier()\nmodel.fit(X, y)\ny_pred = model.predict(X_test)\n\n# Score with RMSE\nprint('Score:', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:56:32.474024Z","iopub.execute_input":"2023-11-12T09:56:32.474406Z","iopub.status.idle":"2023-11-12T09:56:55.889055Z","shell.execute_reply.started":"2023-11-12T09:56:32.474376Z","shell.execute_reply":"2023-11-12T09:56:55.888333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparamater Tuning For Lgbm with Optuna","metadata":{}},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ndef objective(trial):\n    # Split the data into training and validation sets\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Define hyperparameters to optimize\n    params = {\n        'objective': 'binary',\n        'metric': 'binary_error',  # You can change the metric based on your preference\n        'boosting_type': 'gbdt',\n        'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\n    # Create a LightGBM model\n    model = lgb.LGBMClassifier(**params)\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions on the validation set\n    y_pred = model.predict(X_valid)\n\n    # Calculate accuracy for the validation set\n    accuracy = accuracy_score(y_valid, y_pred)\n\n    return 1.0 - accuracy  # Optuna minimizes the objective function, so we use 1.0 - accuracy\n\n# Create a study object and optimize the objective function\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=30)\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint('Best Hyperparameters:', best_params)\n\n# Train the final model with the best hyperparameters on the full dataset\nfinal_model = lgb.LGBMClassifier(**best_params)\nfinal_model.fit(X, y)\n\n# Make predictions on the test set\ny_pred_test = final_model.predict(X_test)\n\n# Evaluate the final model on the test set\ntest_accuracy = accuracy_score(y_test, y_pred_test)\nprint('Test Accuracy:', test_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T04:15:26.674692Z","iopub.execute_input":"2023-11-12T04:15:26.675153Z","iopub.status.idle":"2023-11-12T04:17:33.415337Z","shell.execute_reply.started":"2023-11-12T04:15:26.675119Z","shell.execute_reply":"2023-11-12T04:17:33.414253Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{}},{"cell_type":"code","source":"feature_importance = model.feature_importances_\n\nfeature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n\n# Sort the DataFrame by importance in descending order\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)[:10]\n\n# Display the feature importance\nprint('\\nFeature Importance:')\nprint(feature_importance_df)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nplt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])\nplt.xlabel('Feature')\nplt.ylabel('Importance')\nplt.title('Feature Importance')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T10:03:36.250123Z","iopub.execute_input":"2023-11-12T10:03:36.250508Z","iopub.status.idle":"2023-11-12T10:03:36.633499Z","shell.execute_reply.started":"2023-11-12T10:03:36.250479Z","shell.execute_reply":"2023-11-12T10:03:36.632634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test=processed_test_df.drop(columns=[\"ID\"])\n#test = power_transformer.fit_transform(test)\n\n# Generate the test set predictions\n\ntest_preds = model.predict(test)\n\n# Create a submission file using the predictions of the Random Forest model\nlgb_submission= samplesubmission.copy()\nlgb_submission[\"target\"]= test_preds","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:58:07.163778Z","iopub.execute_input":"2023-11-12T09:58:07.164726Z","iopub.status.idle":"2023-11-12T09:58:07.283264Z","shell.execute_reply.started":"2023-11-12T09:58:07.164690Z","shell.execute_reply":"2023-11-12T09:58:07.282410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_submission.to_csv(\"final_sub_lgb_allX.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:58:38.550308Z","iopub.execute_input":"2023-11-12T09:58:38.551235Z","iopub.status.idle":"2023-11-12T09:58:38.559362Z","shell.execute_reply.started":"2023-11-12T09:58:38.551188Z","shell.execute_reply":"2023-11-12T09:58:38.558440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_submission[\"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-12T09:58:16.875978Z","iopub.execute_input":"2023-11-12T09:58:16.876693Z","iopub.status.idle":"2023-11-12T09:58:16.886501Z","shell.execute_reply.started":"2023-11-12T09:58:16.876660Z","shell.execute_reply":"2023-11-12T09:58:16.885502Z"},"trusted":true},"execution_count":null,"outputs":[]}]}